{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import numpy.random as rng\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ultra\\Anaconda3\\envs\\ocr_env\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, AvgPool2D, BatchNormalization, Dropout, Activation, MaxPool2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from face_rec import Face_Embedding\n",
    "from face_rec import Image_Preprocessing\n",
    "import face_recognition\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조\n",
    "- https://datascienceschool.net/view-notebook/1bde49133d7d40c0806e78b70513040b/\n",
    "- https://jayhey.github.io/deep%20learning/2018/02/06/saimese_network/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지에서 얼굴만 빼고+회전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald_10...\n",
      "img : donald_glover_1.jpg\n",
      "img : donald_glover_10.jpg\n",
      "img : donald_glover_2.jpg\n",
      "img : donald_glover_3.jpg\n",
      "img : donald_glover_4.jpg\n",
      "img : donald_glover_5.jpg\n",
      "img : donald_glover_6.jpg\n",
      "img : donald_glover_7.jpg\n",
      "img : donald_glover_8.jpg\n",
      "img : donald_glover_9.jpg\n",
      "emma_10...\n",
      "img : hermi_1.jpg\n",
      "img : hermi_10.jpg\n",
      "img : hermi_2.jpg\n",
      "img : hermi_3.jpg\n",
      "img : hermi_4.jpg\n",
      "img : hermi_5.jpg\n",
      "img : hermi_6.jpg\n",
      "img : hermi_7.jpg\n",
      "img : hermi_8.jpg\n",
      "img : hermi_9.jpg\n",
      "finn_10...\n",
      "img : finn_1.jpg\n",
      "img : finn_10.jpg\n",
      "img : finn_2.jpg\n",
      "img : finn_3.jpg\n",
      "img : finn_4.jpg\n",
      "img : finn_5.jpg\n",
      "img : finn_6.png\n",
      "img : finn_7.jpg\n",
      "img : finn_8.jpg\n",
      "img : finn_9.jpg\n",
      "freddie_10...\n",
      "img : freddie_highmore_1.jpg\n",
      "img : freddie_highmore_10.jpg\n",
      "img : freddie_highmore_2.jpg\n",
      "img : freddie_highmore_3.jpg\n",
      "img : freddie_highmore_4.jpg\n",
      "img : freddie_highmore_5.jpg\n",
      "img : freddie_highmore_6.jpg\n",
      "img : freddie_highmore_7.jpg\n",
      "img : freddie_highmore_8.jpg\n",
      "img : freddie_highmore_9.jpg\n",
      "gong_10...\n",
      "img : gong1.jpg\n",
      "img : gong10.jpg\n",
      "img : gong2.jpg\n",
      "img : gong3.jpg\n",
      "img : gong4.jpg\n",
      "img : gong5.jpg\n",
      "img : gong6.jpg\n",
      "img : gong7.jpg\n",
      "img : gong8.jpg\n",
      "img : gong9.jpg\n",
      "harry_10...\n",
      "img : harry_1.jpg\n",
      "img : harry_10.jpg\n",
      "img : harry_2.jpg\n",
      "img : harry_3.jpg\n",
      "img : harry_4.jpg\n",
      "img : harry_5.jpg\n",
      "img : harry_6.jpg\n",
      "img : harry_7.jpg\n",
      "img : harry_8.jpg\n",
      "img : harry_9.jpg\n",
      "hyangi_10...\n",
      "img : khk10.jpg\n",
      "img : khk11.jpg\n",
      "img : khk12.jpg\n",
      "img : khk13.jpg\n",
      "img : khk14.jpg\n",
      "img : khk15.jpg\n",
      "img : khk16.jpg\n",
      "img : khk2.jpg\n",
      "img : khk3.jpg\n",
      "img : khk4.jpg\n",
      "img : khk5.jpg\n",
      "img : khk6.jpg\n",
      "img : khk7.jpg\n",
      "img : khk8.jpg\n",
      "img : khk9.jpg\n",
      "jaden_10...\n",
      "img : jaden_1.jpg\n",
      "img : jaden_10.jpg\n",
      "img : jaden_2.jpg\n",
      "img : jaden_3.jpg\n",
      "img : jaden_4.jpg\n",
      "img : jaden_5.jpg\n",
      "img : jaden_6.jpg\n",
      "img : jaden_7.jpg\n",
      "img : jaden_8.jpg\n",
      "img : jaden_9.jpg\n",
      "kim_10...\n",
      "img : kim1.jpg\n",
      "img : kim10.jpg\n",
      "img : kim2.jpg\n",
      "img : kim3.jpg\n",
      "img : kim4.jpg\n",
      "img : kim5.jpg\n",
      "img : kim6.jpg\n",
      "img : kim7.jpg\n",
      "img : kim8.jpg\n",
      "img : kim9.jpg\n",
      "maisie_10...\n",
      "img : maisie_williams_1.jpg\n",
      "img : maisie_williams_10.jpg\n",
      "img : maisie_williams_2.jpg\n",
      "img : maisie_williams_3.jpg\n",
      "img : maisie_williams_4.jpg\n",
      "img : maisie_williams_5.jpg\n",
      "img : maisie_williams_6.jpg\n",
      "img : maisie_williams_7.jpg\n",
      "img : maisie_williams_8.jpg\n",
      "img : maisie_williams_9.jpg\n",
      "matilda_10...\n",
      "img : matilda_1.jpg\n",
      "img : matilda_10.jpg\n",
      "img : matilda_2.jpg\n",
      "img : matilda_3.jpg\n",
      "img : matilda_4.jpg\n",
      "img : matilda_5.jpg\n",
      "img : matilda_6.jpg\n",
      "img : matilda_7.jpg\n",
      "img : matilda_8.jpg\n",
      "img : matilda_9.jpg\n",
      "natalie_10...\n",
      "img : natalie_protman_1.jpg\n",
      "img : natalie_protman_10.jpg\n",
      "img : natalie_protman_2.jpg\n",
      "img : natalie_protman_3.jpg\n",
      "img : natalie_protman_4.jpg\n",
      "img : natalie_protman_5.jpg\n",
      "img : natalie_protman_6.jpg\n",
      "img : natalie_protman_7.jpg\n",
      "img : natalie_protman_8.jpg\n",
      "img : natalie_protman_9.jpg\n",
      "park_10...\n",
      "img : park1.jpg\n",
      "img : park10.jpg\n",
      "img : park2.jpg\n",
      "img : park3.jpg\n",
      "img : park4.jpg\n",
      "img : park6.jpg\n",
      "img : park7.jpg\n",
      "img : park8.jpg\n",
      "img : park9.jpg\n",
      "ron_10...\n",
      "img : ron_weasley_1.jpg\n",
      "img : ron_weasley_10.jpg\n",
      "img : ron_weasley_2.jpg\n",
      "img : ron_weasley_3.jpg\n",
      "img : ron_weasley_4.jpg\n",
      "img : ron_weasley_5.jpg\n",
      "img : ron_weasley_6.jpg\n",
      "error img : ron_weasley_6.jpg\n",
      "img : ron_weasley_7.jpg\n",
      "img : ron_weasley_8.jpg\n",
      "img : ron_weasley_9.jpg\n",
      "sin_10...\n",
      "img : sin1.jpg\n",
      "img : sin10.jpg\n",
      "img : sin2.jpg\n",
      "img : sin3.jpg\n",
      "img : sin4.jpg\n",
      "img : sin5.jpg\n",
      "img : sin6.jpg\n",
      "img : sin7.jpg\n",
      "img : sin8.jpg\n",
      "img : sin9.jpg\n",
      "wang_10...\n",
      "img : wang1.jpg\n",
      "img : wang10.jpg\n",
      "img : wang2.jpg\n",
      "img : wang3.jpg\n",
      "img : wang4.jpg\n",
      "img : wang5.jpg\n",
      "img : wang6.jpg\n",
      "img : wang7.jpg\n",
      "img : wang8.jpg\n",
      "img : wang9.jpg\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folder_dir = './make_images/'\n",
    "# 각 class 폴더 명\n",
    "face_class = os.listdir(folder_dir)\n",
    "ip = Image_Preprocessing()\n",
    "for label, class_ in enumerate(face_class):\n",
    "    class_dir = os.path.join(folder_dir, class_)\n",
    "    write_path = os.path.join(class_dir, class_+'_facebox')\n",
    "    if os.path.exists(write_path):\n",
    "        shutil.rmtree(write_path)\n",
    "    os.mkdir(write_path)\n",
    "    print(\"{}...\".format(class_))\n",
    "    for img in os.listdir(class_dir):\n",
    "        if '.jpg' in img or '.png' in img:\n",
    "            print('img : {}'.format(img))\n",
    "            try:\n",
    "                img_path = os.path.join(class_dir, img)\n",
    "                file_name = os.path.split(img_path)[-1].split('.')[0]\n",
    "                img_ = ip.face_preprocessing(img_path, True)\n",
    "                if img_ != 0:\n",
    "                    cv2.imwrite(os.path.join(write_path, file_name+'_box.jpg'), img_[0])\n",
    "            except:\n",
    "                print('error img : {}'.format(img))\n",
    "                pass\n",
    "            \n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 얼굴만 뽑아낸 이미지를 다른 폴더로 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./make_images/donald_10\\\\donald_10_facebox',\n",
       " './make_images/emma_10\\\\emma_10_facebox',\n",
       " './make_images/finn_10\\\\finn_10_facebox',\n",
       " './make_images/freddie_10\\\\freddie_10_facebox',\n",
       " './make_images/gong_10\\\\gong_10_facebox',\n",
       " './make_images/harry_10\\\\harry_10_facebox',\n",
       " './make_images/hyangi_10\\\\hyangi_10_facebox',\n",
       " './make_images/jaden_10\\\\jaden_10_facebox',\n",
       " './make_images/kim_10\\\\kim_10_facebox',\n",
       " './make_images/maisie_10\\\\maisie_10_facebox',\n",
       " './make_images/matilda_10\\\\matilda_10_facebox',\n",
       " './make_images/natalie_10\\\\natalie_10_facebox',\n",
       " './make_images/park_10\\\\park_10_facebox',\n",
       " './make_images/ron_10\\\\ron_10_facebox',\n",
       " './make_images/sin_10\\\\sin_10_facebox',\n",
       " './make_images/wang_10\\\\wang_10_facebox']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_list = []\n",
    "image_folder = './make_images/'\n",
    "for c in os.listdir(image_folder):\n",
    "    c_p = os.path.join(image_folder, c)\n",
    "    for face_ in os.listdir(c_p):\n",
    "        if 'facebox' in face_:\n",
    "            face_list.append(os.path.join(c_p,face_))\n",
    "face_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 179 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path_ = './small_traiging_images3/'\n",
    "for face_ in face_list:\n",
    "    mkdir_path = os.path.join(path_, os.path.split(face_)[-1])\n",
    "    shutil.copytree(face_, mkdir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 쌍 만들기\n",
    "- (Anchor, Positive), (Anchor, Negative)\n",
    "    - [(batch, 200, 200, 3), (batch, 200, 200, 3)]\n",
    "- siames model에서 사용되는 이미지 쌍들\n",
    "    - 삼중한 손실을 통해 학습을 진행\n",
    "    - 학습이 잘 되게 하기위해서는 같은이미지의 거리와 다른이미지의 거리 사이가 크게 나오게해야함\n",
    "    - 그렇게 학습되게 하려면 d(A, P)와 d(A, N) 사이의 차이가 커야하고\n",
    "        - A와 N을 유사한 이미지를 씀으로써 좀 더 학습이 잘되게 해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...\n",
      "len(AP_list) : 45\n",
      "len(AN_list) : 1530\n",
      "1...\n",
      "len(AP_list) : 90\n",
      "len(AN_list) : 2960\n",
      "2...\n",
      "len(AP_list) : 135\n",
      "len(AN_list) : 4290\n",
      "3...\n",
      "len(AP_list) : 180\n",
      "len(AN_list) : 5520\n",
      "4...\n",
      "len(AP_list) : 225\n",
      "len(AN_list) : 6650\n",
      "5...\n",
      "len(AP_list) : 270\n",
      "len(AN_list) : 7680\n",
      "6...\n",
      "len(AP_list) : 375\n",
      "len(AN_list) : 9000\n",
      "7...\n",
      "len(AP_list) : 420\n",
      "len(AN_list) : 9780\n",
      "8...\n",
      "len(AP_list) : 465\n",
      "len(AN_list) : 10460\n",
      "9...\n",
      "len(AP_list) : 510\n",
      "len(AN_list) : 11040\n",
      "10...\n",
      "len(AP_list) : 555\n",
      "len(AN_list) : 11520\n",
      "11...\n",
      "len(AP_list) : 600\n",
      "len(AN_list) : 11900\n",
      "12...\n",
      "len(AP_list) : 636\n",
      "len(AN_list) : 12161\n",
      "13...\n",
      "len(AP_list) : 672\n",
      "len(AN_list) : 12341\n",
      "14...\n",
      "len(AP_list) : 717\n",
      "len(AN_list) : 12441\n",
      "15...\n",
      "len(AP_list) : 762\n",
      "len(AN_list) : 12441\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image_folder = './small_traiging_images3/'\n",
    "class_folder = os.listdir(image_folder)\n",
    "class_dict = {}\n",
    "image_size = 128\n",
    "AP_list = []\n",
    "AN_list = []\n",
    "for folder, f in enumerate(class_folder):\n",
    "    folder_path = os.path.join(image_folder,f)\n",
    "    class_images = os.listdir(folder_path)\n",
    "    print(\"{}...\".format(folder))\n",
    "    for i, images in enumerate(class_images):\n",
    "        if '.jpg' in images or '.png' in images:\n",
    "            img_path = os.path.join(folder_path, images)\n",
    "            \n",
    "            # Anchor image\n",
    "            img_A = cv2.resize(cv2.imread(img_path), (image_size,image_size)) \n",
    "            for pare_idx in range(i+1,len(class_images)):\n",
    "                pare_path = os.path.join(folder_path, class_images[pare_idx])\n",
    "                # Positive image\n",
    "                img_P = cv2.resize(cv2.imread(img_path), (image_size,image_size)) \n",
    "                AP_list.append([img_A, img_P])\n",
    "    \n",
    "            for folder_idx in range(folder+1, len(class_folder)):\n",
    "                pare_path = os.path.join(image_folder, class_folder[folder_idx])\n",
    "                for negative_img in os.listdir(pare_path):\n",
    "                    img_N_path = os.path.join(pare_path, negative_img)\n",
    "                    img_N = cv2.resize(cv2.imread(img_N_path), (image_size,image_size)) \n",
    "                    AN_list.append([img_A, img_N])\n",
    "    print(\"len(AP_list) : {}\".format(len(AP_list)))\n",
    "    print(\"len(AN_list) : {}\".format(len(AN_list)))\n",
    "class_dict[0] = AN_list\n",
    "class_dict[1] = AP_list      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input data set\n",
    "- [(batch, 128, 128, 3), (batch, 128, 128, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...\n",
      "1...\n",
      "2...\n",
      "3...\n",
      "4...\n",
      "5...\n",
      "Wall time: 43.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image_folder = './training_images/'\n",
    "class_folder = os.listdir(image_folder)\n",
    "class_dict = {}\n",
    "equl_list = []\n",
    "diff_list = []\n",
    "for folder, f in enumerate(class_folder):\n",
    "    folder_path = os.path.join(image_folder,f)\n",
    "    class_images = os.listdir(folder_path)\n",
    "    print(\"{}...\".format(folder))\n",
    "    for i, images in enumerate(class_images):\n",
    "        if '.jpg' in images or '.png' in images:\n",
    "            img_path = os.path.join(folder_path, images)\n",
    "            # 랜덤으로 쌍을 만드는데\n",
    "            pare_image_idx = np.random.randint(len(class_images))\n",
    "            # 자기자신을 선택하면 다시 선택\n",
    "            while(pare_image_idx == i):\n",
    "                pare_image_idx = np.random.randint(len(class_images))\n",
    "            pare_path = os.path.join(folder_path, class_images[pare_image_idx])\n",
    "            img1 = cv2.resize(cv2.imread(img_path), (128,128))\n",
    "            img2 = cv2.resize(cv2.imread(pare_path), (128,128))\n",
    "            pare_folder_idx = np.random.randint(len(class_folder))\n",
    "            while(pare_folder_idx == folder):\n",
    "                pare_folder_idx = np.random.randint(len(class_folder))\n",
    "            # 지금 이 폴더 빼고 다른 폴데의 경로를 가지고옴 \n",
    "            diff_folder_path = os.path.join(image_folder, class_folder[pare_folder_idx])\n",
    "            # 그 폴더내의 이미지목록을 가지고옴\n",
    "            diff_iamges = os.listdir(diff_folder_path)\n",
    "            # 랜덤하게 하나 뽑고\n",
    "            diff_pare_idx = np.random.randint(len(diff_iamges))\n",
    "            diff_image_path = os.path.join(diff_folder_path, diff_iamges[diff_pare_idx])\n",
    "            img3 = cv2.resize(cv2.imread(diff_image_path), (128,128))\n",
    "            # 두 이미지를 rgb채널뒤에 얹어서 더함\n",
    "            # (96,96,6)의 shape으로 만듬\n",
    "#             equl_list.append(np.concatenate((img1, img2), axis = 2))\n",
    "#             diff_list.append(np.concatenate((img1, img3), axis = 2))\n",
    "            equl_list.append([img1, img2])\n",
    "            diff_list.append([img1, img3])\n",
    "#             print(\"img1 : {} img2 : {} img3 : {}\".format(str.split(images, '.')[0],\n",
    "#                                                          str.split(class_images[pare_image_idx], '.')[0],\n",
    "#                                                          str.split(diff_iamges[diff_pare_idx], '.')[0]))\n",
    "            \n",
    "class_dict[0] = diff_list\n",
    "class_dict[1] = equl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from multiprocessing import Pool\n",
    "# import np_preprocessing\n",
    "\n",
    "# np_shape = 200\n",
    "# np_1 = np.zeros((1,np_shape,np_shape,3))\n",
    "# np_2 = np.zeros((1,np_shape,np_shape,3))\n",
    "# if __name__ ==  '__main__': \n",
    "#     with Pool(8) as p:\n",
    "#         np_1, np_2 = p.starmap(np_preprocessing.np_append,[(np_1, np_2, class_dict,np_shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# %%time\n",
    "# def np_append(np_1,np_2, class_dict,np_shape=200):\n",
    "#     np_1 = np.zeros((1,np_shape,np_shape,3))\n",
    "#     np_2 = np.zeros((1,np_shape,np_shape,3))\n",
    "#     for k in class_dict.keys():\n",
    "#         for i, arr_list in enumerate(class_dict[k]):\n",
    "#             if i % 20 == 0:\n",
    "#                 print('{} : {} / {} ...'.format(k, i, len(class_dict[k])))\n",
    "#             for i_, np_arr in enumerate(arr_list):\n",
    "#                 img_ = np_arr.reshape((-1,np_shape,np_shape,3))\n",
    "#                 if i_ == 0:\n",
    "#                     np_1 = np.append(np_1, img_, axis = 0)\n",
    "#                 else:\n",
    "#                     np_2 = np.append(np_2, img_, axis = 0)\n",
    "#     return np_1, np_2\n",
    "\n",
    "# # 분산처리 작업\n",
    "# with Parallel(n_jobs=10, verbose=1, prefer ='threads') as ex:\n",
    "#     # Image column을 분리해서 list로 만들고 np.array로 바꿈\n",
    "#     np_1_test, np_2_test = ex(delayed(np_append)(np_1, np_2, class_dict, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0 / 1750 ...\n",
      "0 : 20 / 1750 ...\n",
      "0 : 40 / 1750 ...\n",
      "0 : 60 / 1750 ...\n",
      "0 : 80 / 1750 ...\n",
      "0 : 100 / 1750 ...\n",
      "0 : 120 / 1750 ...\n",
      "0 : 140 / 1750 ...\n",
      "0 : 160 / 1750 ...\n",
      "0 : 180 / 1750 ...\n",
      "0 : 200 / 1750 ...\n",
      "0 : 220 / 1750 ...\n",
      "0 : 240 / 1750 ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   4692\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4693\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4694\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "np_1 = np.zeros((1,np_shape,np_shape,3))\n",
    "np_2 = np.zeros((1,np_shape,np_shape,3))\n",
    "for k in class_dict.keys():\n",
    "    for i, arr_list in enumerate(class_dict[k]):\n",
    "        if i % 20 == 0:\n",
    "            print('{} : {} / {} ...'.format(k, i, len(class_dict[k])))\n",
    "        for i_, np_arr in enumerate(arr_list):\n",
    "            img_ = np_arr.reshape((-1,np_shape,np_shape,3))\n",
    "            if i_ == 0:\n",
    "                np_1 = np.append(np_1, img_, axis = 0)\n",
    "            else:\n",
    "                np_2 = np.append(np_2, img_, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자기 이외에 다른 이미지 다 잇기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 dict 형태  \n",
    "class_dict[0] = [np.array, np.array, ...] 각 np.array는 (128, 128, 6)  \n",
    "아래의 dict 형태  \n",
    "class_dict[0] = [[np.array, np.array], [np.array, np.array], ...]  각 np.array는 (128,128,3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_dict[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np.append\n",
    "- [(batch, 128, 128, 3), (batch, 128, 128, 3)] 형태의 numpy array만들기 위해 \n",
    "- [ [np.arr, np.arr], [np.arr, np.arr]] 을 뜯어서 아래와 같이 append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_1 = np_1[1:]\n",
    "np_2 = np_2[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_1 shape : (2080, 200, 200, 3) np_2 shape : (2080, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"np_1 shape : {} np_2 shape : {}\".format(np_1.shape, np_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2080,)\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_data = []\n",
    "for k in class_dict.keys():\n",
    "    for _ in class_dict[k]:\n",
    "        y_data.append(k)\n",
    "y_data = np.array(y_data)\n",
    "print(y_data.shape)\n",
    "print(y_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_one_hot = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 np save\n",
    "np.savez('./200_cnn_np', x_data1 = np_1, x_data2 = np_2, y_data = y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('./200_cnn_np2', x_data1 = np_1, x_data2 = np_2, y_data = y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN layer를 2개만듬\n",
    "- 각 layer의 마지막단계에서 나오게되는 특징값들을 펼치고\n",
    "- distance metrics를 구함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "른 레이어(Tensor) 객체를 입력으로 호출하면 그 레이어를 입력으로 가지는 복합 레이어 객체가 된다.\n",
    "Model 클래스 객체는 Input 레이어와 그 Input 레이어에 연결된 다른 레이어를 출력으로 주어 생성.\n",
    "Model 클래스 객체도 다른 레이어(텐서)를 입력으로 호출하면 그 레이어를 입력으로 가지는 복합 레이어가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./200_cnn_np.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./200_cnn_np2.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_1.shape : (660, 200, 200, 3) np_2.shape: (660, 200, 200, 3), y_data.shape : (660,)\n"
     ]
    }
   ],
   "source": [
    "np_1 = data['x_data1']\n",
    "np_2 = data['x_data2']\n",
    "y_data = data['y_data']\n",
    "print(\"np_1.shape : {} np_2.shape: {}, y_data.shape : {}\".format(np_1.shape, np_2.shape, y_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "np_1_norm = scaler.fit_transform(np_1.reshape(-1, 200*200*3))\n",
    "np_2_norm = scaler.fit_transform(np_2.reshape(-1, 200*200*3))\n",
    "np_1_norm = np_1_norm.reshape(-1,200,200,3)\n",
    "np_2_norm = np_2_norm.reshape(-1,200,200,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(np_1, np_2, y_data, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(np_1_norm, np_2_norm, y_data, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "# split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data,y_data,test_size = 0.2, random_state = 42)\n",
    "print(\"x_train.shape :{}\\nx_val.shape : {}\".format(x_train.shape, x_val.shape))\n",
    "print(\"y_train.shape :{}\\ny_val.shape : {}\".format(y_train.shape, y_val.shape))\n",
    "\n",
    "# 데이터 정규화\n",
    "x_train_norm = x_train[:,:,:,:]\n",
    "x_val_norm = x_val[:,:,:,:]\n",
    "x_train_norm = x_train_norm.reshape([-1, img_size*img_size*img_shape])\n",
    "x_val_norm = x_val_norm.reshape([-1,img_size*img_size*img_shape])\n",
    "print(\"x_train_norm.shape : {}\".format(x_train_norm.shape))\n",
    "print(\"x_val_norm,.shape : {}\".format(x_val_norm.shape))\n",
    "x_train_norm = scaler.fit_transform(x_train_norm)\n",
    "x_val_norm = scaler.fit_transform(x_val_norm)\n",
    "print(\"x_train_norm.mean : {}\".format(x_train_norm.mean()))\n",
    "print(\"x_train_norm.std : {}\".format(x_train_norm.std()))\n",
    "# reshape\n",
    "x_train_norm = x_train_norm.reshape([-1,img_size,img_size,img_shape])\n",
    "x_val_norm = x_val_norm.reshape([-1,img_size,img_size,img_shape])\n",
    "print(\"x_train_norm.shape : {}\".format(x_train_norm.shape))\n",
    "print(\"x_val_norm,.shape : {}\".format(x_val_norm.shape))\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "print(\"y_train.shape {}, y_val.shape : {}\".format(y_train.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_train.shape : (528, 200, 200, 3)\n",
      "x1_val.shape : (132, 200, 200, 3)\n",
      "x2_train.shape : (528, 200, 200, 3)\n",
      "x2_val.shape : (132, 200, 200, 3) \n",
      "y_train.shape : (528,)\n",
      "y_val.shape : (132,)\n"
     ]
    }
   ],
   "source": [
    "print(\"x1_train.shape : {}\\nx1_val.shape : {}\\nx2_train.shape : {}\\nx2_val.shape : {} \".format(x1_train.shape, x1_val.shape, x2_train.shape, x2_val.shape))\n",
    "print(\"y_train.shape : {}\\ny_val.shape : {}\".format(y_train.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본\n",
    "- weight, bias init => he_normal\n",
    "- 링크 : https://github.com/sorenbouma/keras-oneshot/blob/master/SiameseNet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# def W_init(shape,name=None):\n",
    "#     \"\"\"Initialize weights as in paper\"\"\"\n",
    "#     values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
    "#     return K.variable(values,name=name)\n",
    "# #//TODO: figure out how to initialize layer biases in keras.\n",
    "# def b_init(shape,name=None):\n",
    "#     \"\"\"Initialize bias as in paper\"\"\"\n",
    "#     values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
    "#     return K.variable(values,name=name)\n",
    "\n",
    "input_shape = (128, 128, 3)\n",
    "left_input = Input(input_shape)\n",
    "right_input = Input(input_shape)\n",
    "#build convnet to use in each siamese 'leg'\n",
    "convnet = Sequential()\n",
    "#convnet.add(Conv2D(48,(10,10),activation='relu',input_shape=input_shape,kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
    "convnet.add(Conv2D(48,(10,10),activation = 'relu', input_shape=input_shape,kernel_initializer='he_normal',kernel_regularizer=l2(2e-4)))\n",
    "\n",
    "convnet.add(MaxPooling2D())\n",
    "#convnet.add(Conv2D(36,(7,7),activation='relu',kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
    "convnet.add(Conv2D(36,(7,7),activation = 'relu',kernel_regularizer=l2(2e-4),kernel_initializer='he_normal',bias_initializer='he_normal'))\n",
    "\n",
    "convnet.add(MaxPooling2D())\n",
    "#convnet.add(Conv2D(96,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "convnet.add(Conv2D(96,(4,4),activation = 'relu', kernel_initializer='he_normal',kernel_regularizer=l2(2e-4),bias_initializer='he_normal'))\n",
    "\n",
    "convnet.add(MaxPooling2D())\n",
    "#convnet.add(Conv2D(64,(3,3),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
    "#convnet.add(Conv2D(48,(3,3),activation='relu',kernel_initializer='he_normal',kernel_regularizer=l2(2e-4),bias_initializer='he_normal'))\n",
    "\n",
    "#convnet.add(Dropout(0.7))\n",
    "convnet.add(Flatten())\n",
    "convnet.add(Dense(512,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer='he_normal',bias_initializer='he_normal'))\n",
    "#convnet.add(Dense(1024,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer='he_normal',bias_initializer='he_normal'))\n",
    "#convnet.add(Dense(128,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer='he_normal',bias_initializer='he_normal'))\n",
    "\n",
    "#call the convnet Sequential model on each of the input tensors so params will be shared\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "#layer to merge two encoded inputs with the l1 distance between them\n",
    "# 임의의 표현식을 Layer객체로 래핑(여기선 CNN거치고 나온 각 이미지가 서로의 차를 송출하는 layer를 만듬)\n",
    "# keras는 tensorflow의텐서 곱셈이라든지 같은 저수준 연산을 처리하지 않음\n",
    "# 하지만 keras의 backend를 통해 tensorflow의 연산을 수행할 수 있음(여기선 텐서간 차를 구하고 절대값)\n",
    "L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "#call this layer on list of two input tensors.\n",
    "L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "#add_dense = Dense(84, activation='relu',kernel_regularizer=l2(1e-3),kernel_initializer='he_normal',bias_initializer='he_normal')(L1_distance)\n",
    "prediction = Dense(1,activation='sigmoid',bias_initializer='he_normal')(L1_distance)\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "\n",
    "optimizer = Adam(0.00006)\n",
    "#optimzer = keras.optimizers.SGD(lr=0.0001)\n",
    "#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworking\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 512)          6102452     input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 512)          0           sequential_5[1][0]               \n",
      "                                                                 sequential_5[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            513         lambda_5[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,102,965\n",
      "Trainable params: 6,102,965\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " [<tf.Tensor 'input_9:0' shape=(?, 128, 128, 3) dtype=float32>,\n",
       "  <tf.Tensor 'input_10:0' shape=(?, 128, 128, 3) dtype=float32>],\n",
       " <tf.Tensor 'dense_10/Sigmoid:0' shape=(?, 1) dtype=float32>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.summary(), siamese_net.input, siamese_net.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [x1_train, x2_train]\n",
    "val_data = [[x1_val, x2_val], y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1219"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1219 samples, validate on 305 samples\n",
      "Epoch 1/30\n",
      " 850/1219 [===================>..........] - ETA: 39s - loss: 1.6071 - acc: 0.5094"
     ]
    }
   ],
   "source": [
    "log = siamese_net.fit(input_data, y_train, epochs = 30, batch_size = 2, validation_data= val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(*logs):\n",
    "    trn_loss, val_loss, trn_acc, val_acc = [],[],[],[]\n",
    "    \n",
    "    for log in logs:\n",
    "        trn_loss += log.history['loss']\n",
    "        val_loss += log.history['val_loss']\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize = (8,4))\n",
    "    # loss value\n",
    "    ax.plot(trn_loss, label = 'train')\n",
    "    # accuracy\n",
    "    ax.plot(val_loss, label = 'validation')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img2(img_path):\n",
    "    img_size = 200\n",
    "    return cv2.resize(cv2.imread(img_path), (img_size,img_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_path = './small_traiging_images2/emma_10_extend/hermi_1_box.jpg'\n",
    "cmp_path1 = './small_traiging_images2/emma_10_extend/hermi_3_box.jpg'\n",
    "cmp_path2 = './small_traiging_images2/finn_10_extend/finn_2_box.jpg'\n",
    "cmp_path3 = './small_traiging_images2/finn_10_extend/finn_7_box.jpg'\n",
    "cmp_path4 = './small_traiging_images2/harry_10_extend/harry_6_box.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_path = './small_traiging_images2/emma_10_extend/hermi_1_box.jpg'\n",
    "cmp_path1 = './small_traiging_images2/emma_10_extend/hermi_2_box.jpg'\n",
    "cmp_path2 = './small_traiging_images2/emma_10_extend/hermi_3_box.jpg'\n",
    "cmp_path3 = './small_traiging_images2/emma_10_extend/hermi_4_box.jpg'\n",
    "cmp_path4 = './small_traiging_images2/emma_10_extend/hermi_5_box.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = resize_img2(ori_path)\n",
    "compared_img1 = resize_img2(cmp_path1)\n",
    "compared_img2 = resize_img2(cmp_path2)\n",
    "compared_img3 = resize_img2(cmp_path3)\n",
    "compared_img4 = resize_img2(cmp_path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = 200\n",
    "original_img = original_img.reshape([-1,img_shape,img_shape,3])\n",
    "compared_img1 = compared_img1.reshape([-1,img_shape,img_shape,3])\n",
    "compared_img2 = compared_img2.reshape([-1,img_shape,img_shape,3])\n",
    "compared_img3 = compared_img3.reshape([-1,img_shape,img_shape,3])\n",
    "compared_img4 = compared_img4.reshape([-1,img_shape,img_shape,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200, 200, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8266766]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.predict([original_img, compared_img1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6332958]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.predict([original_img, compared_img2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6332958]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.predict([original_img, compared_img3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2414935]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_net.predict([original_img, compared_img4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로운 쌍 만들기..(같은 사진과 다른사진의 비율을 같게)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...\n",
      "len(AP_list) : 45\n",
      "len(AN_list) : 45\n",
      "1...\n",
      "len(AP_list) : 90\n",
      "len(AN_list) : 90\n",
      "2...\n",
      "len(AP_list) : 135\n",
      "len(AN_list) : 135\n",
      "3...\n",
      "len(AP_list) : 180\n",
      "len(AN_list) : 180\n",
      "4...\n",
      "len(AP_list) : 225\n",
      "len(AN_list) : 225\n",
      "5...\n",
      "len(AP_list) : 270\n",
      "len(AN_list) : 270\n",
      "6...\n",
      "len(AP_list) : 375\n",
      "len(AN_list) : 375\n",
      "7...\n",
      "len(AP_list) : 420\n",
      "len(AN_list) : 420\n",
      "8...\n",
      "len(AP_list) : 465\n",
      "len(AN_list) : 465\n",
      "9...\n",
      "len(AP_list) : 510\n",
      "len(AN_list) : 510\n",
      "10...\n",
      "len(AP_list) : 555\n",
      "len(AN_list) : 555\n",
      "11...\n",
      "len(AP_list) : 600\n",
      "len(AN_list) : 600\n",
      "12...\n",
      "len(AP_list) : 636\n",
      "len(AN_list) : 636\n",
      "13...\n",
      "len(AP_list) : 672\n",
      "len(AN_list) : 672\n",
      "14...\n",
      "len(AP_list) : 717\n",
      "len(AN_list) : 717\n",
      "15...\n",
      "len(AP_list) : 762\n",
      "len(AN_list) : 762\n",
      "Wall time: 4.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image_folder = './small_traiging_images3/'\n",
    "class_folder = os.listdir(image_folder)\n",
    "image_shape = 128\n",
    "class_dict = {}\n",
    "AP_list = []\n",
    "AN_list = []\n",
    "for folder, f in enumerate(class_folder):\n",
    "    folder_path = os.path.join(image_folder,f)\n",
    "    class_images = os.listdir(folder_path)\n",
    "    print(\"{}...\".format(folder))\n",
    "    for i, images in enumerate(class_images):\n",
    "        if '.jpg' in images or '.png' in images:\n",
    "            img_path = os.path.join(folder_path, images)\n",
    "            \n",
    "            # Anchor image\n",
    "            img_A = cv2.resize(cv2.imread(img_path), (image_shape,image_shape)) \n",
    "            for pare_idx in range(i+1,len(class_images)):\n",
    "                pare_path = os.path.join(folder_path, class_images[pare_idx])\n",
    "                # Positive image\n",
    "                img_P = cv2.resize(cv2.imread(pare_path), (image_shape,image_shape)) \n",
    "                AP_list.append([img_A, img_P])\n",
    "                # Negative image\n",
    "                pare_folder_idx = np.random.randint(len(class_folder))\n",
    "                while(pare_folder_idx == folder):\n",
    "                    pare_folder_idx = np.random.randint(len(class_folder))\n",
    "                # 지금 이 폴더 빼고 다른 폴데의 경로를 가지고옴 \n",
    "                diff_folder_path = os.path.join(image_folder, class_folder[pare_folder_idx])\n",
    "                # 그 폴더내의 이미지목록을 가지고옴\n",
    "                diff_iamges = os.listdir(diff_folder_path)\n",
    "                # 랜덤하게 하나 뽑고\n",
    "                diff_pare_idx = np.random.randint(len(diff_iamges))\n",
    "                diff_image_path = os.path.join(diff_folder_path, diff_iamges[diff_pare_idx])\n",
    "                img_N = cv2.resize(cv2.imread(diff_image_path), (image_shape,image_shape))\n",
    "                AN_list.append([img_A, img_N])\n",
    "            \n",
    "                    \n",
    "    print(\"len(AP_list) : {}\".format(len(AP_list)))\n",
    "    print(\"len(AN_list) : {}\".format(len(AN_list)))\n",
    "class_dict[0] = AN_list\n",
    "class_dict[1] = AP_list      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0 / 762 ...\n",
      "0 : 50 / 762 ...\n",
      "0 : 100 / 762 ...\n",
      "0 : 150 / 762 ...\n",
      "0 : 200 / 762 ...\n",
      "0 : 250 / 762 ...\n",
      "0 : 300 / 762 ...\n",
      "0 : 350 / 762 ...\n",
      "0 : 400 / 762 ...\n",
      "0 : 450 / 762 ...\n",
      "0 : 500 / 762 ...\n",
      "0 : 550 / 762 ...\n",
      "0 : 600 / 762 ...\n",
      "0 : 650 / 762 ...\n",
      "0 : 700 / 762 ...\n",
      "0 : 750 / 762 ...\n",
      "1 : 0 / 762 ...\n",
      "1 : 50 / 762 ...\n",
      "1 : 100 / 762 ...\n",
      "1 : 150 / 762 ...\n",
      "1 : 200 / 762 ...\n",
      "1 : 250 / 762 ...\n",
      "1 : 300 / 762 ...\n",
      "1 : 350 / 762 ...\n",
      "1 : 400 / 762 ...\n",
      "1 : 450 / 762 ...\n",
      "1 : 500 / 762 ...\n",
      "1 : 550 / 762 ...\n",
      "1 : 600 / 762 ...\n",
      "1 : 650 / 762 ...\n",
      "1 : 700 / 762 ...\n",
      "1 : 750 / 762 ...\n",
      "Wall time: 6min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np_shape = 128\n",
    "np_1 = np.zeros((1,np_shape,np_shape,3))\n",
    "np_2 = np.zeros((1,np_shape,np_shape,3))\n",
    "for k in class_dict.keys():\n",
    "    for i, arr_list in enumerate(class_dict[k]):\n",
    "        if i % 50 == 0:\n",
    "            print('{} : {} / {} ...'.format(k, i, len(class_dict[k])))\n",
    "        for i_, np_arr in enumerate(arr_list):\n",
    "            img_ = np_arr.reshape((-1,np_shape,np_shape,3))\n",
    "            if i_ == 0:\n",
    "                np_1 = np.append(np_1, img_, axis = 0)\n",
    "            else:\n",
    "                np_2 = np.append(np_2, img_, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_1 = np_1[1:]\n",
    "np_2 = np_2[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1524,)\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_data = []\n",
    "for k in class_dict.keys():\n",
    "    for _ in class_dict[k]:\n",
    "        y_data.append(k)\n",
    "y_data = np.array(y_data)\n",
    "print(y_data.shape)\n",
    "print(y_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 np save\n",
    "np.savez('./128_real_cnn_np', x_data1 = np_1, x_data2 = np_2, y_data = y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 np save\n",
    "np.savez('./200_real_cnn_np', x_data1 = np_1, x_data2 = np_2, y_data = y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_1.shape : (1524, 200, 200, 3) np_2.shape: (1524, 200, 200, 3), y_data.shape : (1524,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./200_real_cnn_np.npz')\n",
    "np_1 = data['x_data1']\n",
    "np_2 = data['x_data2']\n",
    "y_data = data['y_data']\n",
    "print(\"np_1.shape : {} np_2.shape: {}, y_data.shape : {}\".format(np_1.shape, np_2.shape, y_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_1.shape : (1524, 128, 128, 3) np_2.shape: (1524, 128, 128, 3), y_data.shape : (1524,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./128_real_cnn_np.npz')\n",
    "np_1 = data['x_data1']\n",
    "np_2 = data['x_data2']\n",
    "y_data = data['y_data']\n",
    "print(\"np_1.shape : {} np_2.shape: {}, y_data.shape : {}\".format(np_1.shape, np_2.shape, y_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1_train.shape : (1219, 128, 128, 3)\n",
      "x1_val.shape : (305, 128, 128, 3)\n",
      "x2_train.shape : (1219, 128, 128, 3)\n",
      "x2_val.shape : (305, 128, 128, 3) \n",
      "y_train.shape : (1219,)\n",
      "y_val.shape : (305,)\n"
     ]
    }
   ],
   "source": [
    "x1_train, x1_val, x2_train, x2_val, y_train, y_val = train_test_split(np_1, np_2, y_data, test_size = 0.2, random_state = 42)\n",
    "print(\"x1_train.shape : {}\\nx1_val.shape : {}\\nx2_train.shape : {}\\nx2_val.shape : {} \".format(x1_train.shape, x1_val.shape, x2_train.shape, x2_val.shape))\n",
    "print(\"y_train.shape : {}\\ny_val.shape : {}\".format(y_train.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model save/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Creates a HDF5 file 'my_model.h5'\n",
    "siamese_net.save('siamese_net_200_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7392402]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# Returns a compiled model identical to the previous one\n",
    "model = load_model('./siamese_net_200_test.h5')\n",
    "\n",
    "img_size = 200\n",
    "origianl_img = cv2.resize(cv2.imread(ori_path), (img_size,img_size))\n",
    "\n",
    "model.predict(([original_img, compared_img1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200, 200, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resize_reshape_img(original_img_path, 200).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가장 유사한 이미지 이름과 acc 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hermi_10_box.jpg predict...\n",
      "hermi_1_box.jpg predict...\n",
      "hermi_2_box.jpg predict...\n",
      "hermi_3_box.jpg predict...\n",
      "hermi_4_box.jpg predict...\n",
      "hermi_5_box.jpg predict...\n",
      "hermi_6_box.jpg predict...\n",
      "hermi_7_box.jpg predict...\n",
      "hermi_8_box.jpg predict...\n",
      "hermi_9_box.jpg predict...\n",
      "finn_10_box.jpg predict...\n",
      "finn_1_box.jpg predict...\n",
      "finn_2_box.jpg predict...\n",
      "finn_3_box.jpg predict...\n",
      "finn_4_box.jpg predict...\n",
      "finn_5_box.jpg predict...\n",
      "finn_6_box.png predict...\n",
      "finn_7_box.jpg predict...\n",
      "finn_8_box.jpg predict...\n",
      "finn_9_box.jpg predict...\n",
      "harry_10_box.jpg predict...\n",
      "harry_1_box.jpg predict...\n",
      "harry_2_box.jpg predict...\n",
      "harry_3_box.jpg predict...\n",
      "harry_4_box.jpg predict...\n",
      "harry_5_box.jpg predict...\n",
      "harry_6_box.jpg predict...\n",
      "harry_7_box.jpg predict...\n",
      "harry_8_box.jpg predict...\n",
      "harry_9_box.jpg predict...\n",
      "file name : hermi_10_box, acc : [0.7392402]\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# image를 resize해주고 reshape.. 인자로 size를 줘야함\n",
    "def resize_reshape_img(img_path, img_size):\n",
    "    img = cv2.resize(cv2.imread(img_path),(img_size,img_size))\n",
    "    return img.reshape([-1, img_size, img_size, 3])\n",
    "\n",
    "img_path = './small_traiging_images2/'\n",
    "original_img_path = './small_traiging_images2/emma_10_extend/hermi_1_box.jpg'\n",
    "# original img <- 들어오는 이미지 shape과 size를 바꿔줌\n",
    "original_img = resize_reshape_img(original_img_path, 200)\n",
    "\n",
    "acc_list = []\n",
    "file_list = []\n",
    "# 전체경로에서 각 이미지 폴더에 접근\n",
    "for folder in os.listdir(img_path):\n",
    "    folder_path = os.path.join(img_path, folder)\n",
    "    # 각 이미지 폴더에서 폴더내 이미지 접근\n",
    "    for file in os.listdir(folder_path):\n",
    "        print(\"{} predict...\".format(file))\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        # db폴더내의 각 이미지에 대해 resize , reshape\n",
    "        compare_file = resize_reshape_img(file_path, 200)\n",
    "        file_list.append(str.split(file, '.')[0])\n",
    "        acc_list.append(model.predict([original_img, compare_file])[0])\n",
    "acc_np = np.array(acc_list)        \n",
    "max_idx = np.argmax(acc_np)\n",
    "print('file name : {}, acc : {}'.format(file_list[max_idx], acc_np[max_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ocr_env] *",
   "language": "python",
   "name": "conda-env-ocr_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
